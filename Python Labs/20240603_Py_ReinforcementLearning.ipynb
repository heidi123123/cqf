{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad11f2a-d2b4-4335-bf53-05dd130357e9",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial; text-align: center;\">\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "#### Kannan Singaravelu, CQF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283739a-6180-4724-8103-0c6055e00841",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4bbca-ff30-4b88-b829-df56352ddb8a",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is one of three machine learning paradigms, alongside supervised learning and unsupervised learning. RL differs from supervised learning as it focus on finding a balance between exploration and exploitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f5358-479a-4030-bc87-e00ba3854629",
   "metadata": {},
   "source": [
    "In this approach, the (algorithm) agents take actions in an environment in order to maximize its cumulative reward. The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728faba1-a083-44e5-b649-d30c246317ab",
   "metadata": {},
   "source": [
    "MDP is a discrete-time stochastic control process and provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. Such processes are useful in optimization problems solved via dynamic programming and reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f153a85-7a90-499f-95b4-b349f15e1311",
   "metadata": {},
   "source": [
    "RL framework is extremely promising in trading and investment management. Currently RL is mostly a research area and hasn’t yet had significant practical successes beyond games. However, it’s an idea whose time has come.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7408f-63bd-464a-bf10-700eb5299f54",
   "metadata": {},
   "source": [
    "## Fundamental Notions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a04061-b1a0-4220-8230-c90dfdd2c08c",
   "metadata": {},
   "source": [
    "Following are some of the fundamental notions of the RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643901e-f381-4276-985a-d3325e55709a",
   "metadata": {},
   "source": [
    "<img src=\"20240603_Py_ReinforcementLearning1.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd66a1-5b1c-44ab-96b2-8145ec7221af",
   "metadata": {},
   "source": [
    "***Environment*** : The world through which agents moves. The environment defines the problem at hand. For example, the environment can be a financial market to be traded in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296282c-8cb6-4929-a39e-f2fe5e1f0952",
   "metadata": {},
   "source": [
    "***State*** : Current condition returned by the environment. A state subsumes all relevant parameters that describe the current state of the environment. In a financial market, this might include current and historical price levels or financial indicators such as moving averages, macroeconomic variables, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30162af-ccac-476c-8391-9cff76ebe2e6",
   "metadata": {},
   "source": [
    "***Agent*** : The RL algorithm that learns from the exploration and exploitation. Agent subsumes all elements of the RL algorithm that interacts with the environment and learns from these interactions. In a financial context, the agent could represent a trader placing bets on rising or falling markets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13340b18-975e-4026-a13d-5597043b9cb5",
   "metadata": {},
   "source": [
    "***Action*** : All possible steps that the agent can take from a (limited) set of allowed actions. In a financial market, going long or short could be admissible actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc837b1-318c-4844-b055-071117b02bb9",
   "metadata": {},
   "source": [
    "***Reward*** : An instant return from the environment to appraise the last action. In a financial context, profit (or loss) is a standard reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e5f4d-db80-4eb5-b20f-00965fbcc7a0",
   "metadata": {},
   "source": [
    "***Policy*** : The approach the agent uses to determine the next action based on the current state. In a financial context, a trading bot that observes three price increases in a row might decide to buy the market.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02020c2e-39c9-459e-977f-b52019d4e358",
   "metadata": {},
   "source": [
    "***Value*** : The expected long-term return with discount, as opposed to short-term rewards. For a financial trading bot, this might be the expected accumulated trading profit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694202f4-674c-4cfc-bcf8-a073bc53484f",
   "metadata": {},
   "source": [
    "***Episode*** : The set of steps from the initial state of the environment until success is achieved or failure is observed. In the financial world, this might be from the beginning of the year to the end of the year or to bankruptcy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50561da-088d-4188-a84f-9a5ecc1f1dba",
   "metadata": {},
   "source": [
    "## RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbde307-492a-4c3d-9813-413600ed64af",
   "metadata": {},
   "source": [
    "![Description or alt text](20240603_Py_ReinforcementLearning2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec13d41-cee1-495b-9f50-4a0da76a3ca3",
   "metadata": {},
   "source": [
    "A non-exhaustive, but useful taxonomy of algorithms in modern RL [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4318c68-d1aa-4928-911d-cdca6c5518b5",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d64fa0-d874-4664-882b-3ea5d1770cbc",
   "metadata": {},
   "source": [
    "MDP is the mathematical formulation of the RL problem where it uses the Markov property where the current state completely characterises the state of the world and is defined by $(S,A,R,P,\\gamma)$, where\n",
    "\n",
    "- $S$: set of possible states\n",
    "- $A$: set of possible actions\n",
    "- $R$: distribution of reward given (state, action) pair\n",
    "- $P$: transition probability, i.e., distribution over next state given (state, action) pair\n",
    "- $\\gamma$: discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6f40f-4ff0-4647-af5d-0a303f2f90ea",
   "metadata": {},
   "source": [
    "The MDP process is as follows,\n",
    "\n",
    "- Environment samples initial state at $S_0 p(s_0)$ at $t = 0$\n",
    "\n",
    "- Then, for $t = 0$ until done:\n",
    "  - agent selects action $a_t$\n",
    "  - environment samples reward $r_t R(\\cdot \\mid s_t, a_t)$\n",
    "  - environment samples next state $s_{t+1} P(\\cdot \\mid s_t, a_t)$\n",
    "  - agent receives reward $r_t$ and next state $s_{t+1}$\n",
    "\n",
    "- Policy $\\pi$ is a function from $S$ to $A$ that specifies what action to take in each state\n",
    "\n",
    "- Objective: find policy $\\pi^*$ or value $Q^*$ that maximizes cumulative discounted reward: $\\sum_{t>0} \\gamma^t r_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57322126-75c2-4c08-b511-149b8f42de75",
   "metadata": {},
   "source": [
    "## Shortest Path Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74390b99-7b26-4272-a9f5-8829080e4528",
   "metadata": {},
   "source": [
    "<img src=\"20240603_Py_ReinforcementLearning3.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fee7fa-3367-4ef9-8ca3-a93d7a981f3d",
   "metadata": {},
   "source": [
    "The markov decision process is used to find the shortest path between A and D with maximum reward. In this example,\n",
    "* Set of states are denoted by nodes {A,B,C,D}\n",
    "* Action is to traverse from one node to another\n",
    "* Reward is the cost represented by each edge\n",
    "* Policy is the path taken to reach the destination {A >> C >> D}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ea3fb-9432-46f8-8822-a96f28a2dd13",
   "metadata": {},
   "source": [
    "## Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e4c04-60f6-408e-a28d-9e4136614d45",
   "metadata": {},
   "source": [
    "**Q-learning** is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances. Q-learning finds an optimal policy by maximizing the expected value of the total reward over any and all successive steps, starting from the current state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409aa710-673a-44be-92cc-f2ed765b6503",
   "metadata": {},
   "source": [
    "Q-learning is an algorithm that takes into account delayed rewards in addition to immediate rewards from an action. There is an action-value policy Q, which assigns a value to every combination of a state and an action. Higher the value, the better the action from the point of view of the agent. Agent who uses the policy Q to choose an action, selects the action with the highest value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537b074-a2bc-4a96-bb06-0b8010597025",
   "metadata": {},
   "source": [
    "<img src=\"20240603_Py_ReinforcementLearning4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5e9c9-6b53-412f-b82f-3b692ec5d1ca",
   "metadata": {},
   "source": [
    "**How is the value of an action derived?**\n",
    "\n",
    "The value of an action is composed of its direct reward and the discounted value of the optimal action in the next state. The following is the formal expression:\n",
    "\n",
    "The value of an action is composed of its direct reward and the discounted value of the optimal action in the next state. The following is the formal expression:\n",
    "$$Q(S_t, A_t) = R_t + \\gamma \\max_{a} Q(S_{t+1}, a)$$\n",
    "where, $S_t$ is the state at timestep $t$, $A_t$ is the action taken at state $S_t$, $R_t$ is the direct reward of action $A_t$, $\\gamma\\in\\left[0,1\\right]$ is the discount factor, and $\\max_{a} Q(S_{t+1}, a)$ is the maximum delayed reward given the optimal action from the current policy $Q$.\n",
    "\n",
    "To find the optimal policy, we use value iteration algorithm that use the above Bellman equation as an iterative update where $Q_i$ will converge to optimal $Q^*$ as $i \\to \\infty$.\n",
    "\n",
    "Such approach is not scalable as it is computationally infeasible to compute for entire state space. To address this we use a function approximator to estimate the action-value function $Q(s, a; \\theta) \\approx Q^*(s, a)$. If the function approximator is a deep neural network, then we call this as **deep q-leaning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f586325-472a-48bf-9af3-305ca576c742",
   "metadata": {},
   "source": [
    "## Q-Learning Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ec35c-9d61-475f-8d60-c40b8abd2de1",
   "metadata": {},
   "source": [
    "In a simple environment with a limited number of possible states, Q can be represented by a table, where we can list all state-action combination with the corresponding value.\n",
    "\n",
    "Let's initialize $Q$ as a zero matrix and $R$ as below:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "Q = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "&\n",
    "R = \\begin{bmatrix}\n",
    "-1 & -1 & -1 & -1 & 0 & -1 \\\\\n",
    "-1 & -1 & 0 & -1 & -1 & 100 \\\\\n",
    "-1 & -1 & 0 & -1 & -1 & -1 \\\\\n",
    "0 & -1 & -1 & -1 & 100 & -1 \\\\\n",
    "-1 & 0 & -1 & -1 & 0 & -1 \\\\\n",
    "-1 & -1 & -1 & 100 & -1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$\\gamma$ ranges from 0 to 1. $\\gamma \\sim 0$, the agent will consider immediate rewards; $\\gamma \\sim 1$, the agent will consider future rewards with higher weights. For this example, let us consider an initial state of 1 with $\\gamma = 0.80$ to calculate the Q-value.\n",
    "$$Q(\\text{state, action}) = R(\\text{state, action}) + \\gamma \\times \\max[\\{Q(\\text{next state, all actions})\\}]$$\n",
    "\n",
    "$Q(1,5) = R(1,5) + 0.80 \\times \\max[Q(5,1), Q(5,4), Q(5,5)] = 100 + 0.80 = 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3372aaaa-f409-466d-8b66-986ad3f313fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c9bbe5-3973-40fd-a48d-916d22d7b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix \n",
      " \n",
      " [[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [ -1   0   0  -1  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n"
     ]
    }
   ],
   "source": [
    "# R matrix\n",
    "R = np.array([[-1,-1,-1,-1,0,-1],\n",
    "             [-1,-1,-1,0,-1,100],\n",
    "             [-1,-1,-1,0,-1,-1],\n",
    "             [-1,0,0,-1,0,-1],\n",
    "             [-1,0,0,-1,-1,100],\n",
    "             [-1,0,-1,-1,0,100]])\n",
    "\n",
    "print(f'Reward Matrix \\n \\n {R}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fe84bf-431f-4ee7-97a4-2ba24029de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Matrix \n",
      " \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Q Matrix\n",
    "Q = np.array(np.zeros([6,6]))\n",
    "print(f'Q Matrix \\n \\n {Q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcefa90-1e9b-4dad-95e0-78b0ece64e5c",
   "metadata": {},
   "source": [
    "Let's now return all available actions in the state given as an argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88c31a2-a0c4-418c-b2bf-09781c943305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial State - choosen at random\n",
    "initial_state = 1\n",
    "\n",
    "# Gamma (discount paramaters)\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27101de4-01c1-4c13-a41a-5cc95777c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(R[1]>=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfdcf75-c49a-4c84-94e6-03f04d5ef29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now return all available actions in the state given as an argument\n",
    "def available_actions(state):\n",
    "    current_state_row = R[state]\n",
    "    aaction = np.where(current_state_row >=0)[0]\n",
    "    return aaction\n",
    "\n",
    "# Get available actions in the current state\n",
    "available_act = available_actions(initial_state)\n",
    "available_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a22d7f-c27d-4311-b6a7-00f615aeda66",
   "metadata": {},
   "source": [
    "Let's choose which action to be performed (at random) within the range of all available actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50e95bd8-46f0-468e-98cf-e8f2401fe852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next action to be performed\n",
    "def next_action(available_action_range):\n",
    "    naction = int(np.random.choice(available_act,1))\n",
    "    return naction\n",
    "\n",
    "# Action to be performed\n",
    "action = next_action(available_act)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47146f9d-37b3-4e89-ad76-b3450bfb237e",
   "metadata": {},
   "source": [
    "Let's now update the Q Matrix according to the path selected and the Q learning algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f160038-9105-4f18-8b10-6b01f762a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Q Matrix\n",
    "def update(current_state, action, gamma):\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index,size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    \n",
    "    max_value = Q[action, max_index]\n",
    "    \n",
    "    # Q learning formula \n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
    "    \n",
    "# Update Q-matrix\n",
    "update(initial_state, action, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebaa21-62b5-4394-9cdf-820b11d86862",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8f78ea-d8a5-43f6-a5cd-ea084a72cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-Matrix \n",
      " \n",
      " [[  0.    0.    0.    0.   80.    0. ]\n",
      " [  0.    0.    0.   64.    0.  100. ]\n",
      " [  0.    0.    0.   64.    0.    0. ]\n",
      " [  0.   80.   51.2   0.   80.    0. ]\n",
      " [  0.   80.   51.2   0.    0.  100. ]\n",
      " [  0.   80.    0.    0.   80.  100. ]]\n"
     ]
    }
   ],
   "source": [
    "# Training for 10000 iterations\n",
    "\n",
    "for i in range(10000):\n",
    "    current_state = np.random.randint(0,int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = next_action(available_act)\n",
    "    update(current_state, action, gamma)\n",
    "    \n",
    "# Normalize the Q matrix\n",
    "print(f'Trained Q-Matrix \\n \\n {Q/np.max(Q)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b69ab4-4072-4dde-b58d-87cb4b48bc1c",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8bfb3f-a251-4be8-8be9-7e35677a2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = 2\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state !=5:\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[0]\n",
    "    \n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index,size=1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "        \n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef561c-d3bb-4a9c-bad2-959562a1b8a8",
   "metadata": {},
   "source": [
    "### Selectetd Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69abdfda-0b16-4361-ba71-a101efe2d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Path [2, 3, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# Print selected sequence of steps\n",
    "print(f'Selected Path {steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b40ac-c5b8-4785-883d-fa16085546af",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "OpenAI [2] is an organization that facilitate research in RL. OpenAI has developed and open sourced a suite of environments, called `OpenAI Gym` that allows the training of RL agents via a standardized API.\n",
    "\n",
    "The gym library addressed two important gaps that were slowing down the RL research.\n",
    "1. Need for better benchmarks\n",
    "2. Lack of standardization of environments used in publications\n",
    "\n",
    "Gym is a toolkit for developing and comparing RL algorithms and makes no assumptions about the structure of the agent. It is compatible with any numerical computation library, such as TensorFlow or Theano. The gym library is a collection of environments that we can use to work out the RL algorithms. These environments have a shared interface, allowing us to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3b844-9964-4136-8621-b66a40e2d3b3",
   "metadata": {},
   "source": [
    "### Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3b101de-26c5-4fd6-8091-bdd76918d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad5db864-04ed-486c-8404-9cb10868874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f215012-ab64-4bca-922f-afdd7c0ffc5b",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "Here’s an example of cart-pole environment which will run an instance of the CartPole-v0 environment for 1000 timesteps, rendering the environment at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "298dd833-8325-4691-8889-12892a07a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())  # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080a3e6-638e-45a0-88ee-f6ec6547aefb",
   "metadata": {},
   "source": [
    "### Observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017db24c-af7a-4436-a9e8-ea2049d34ecb",
   "metadata": {},
   "source": [
    "Observing the outcome of our action is critical for us to do better than random actions. The environment’s step function returns exactly what we need by returning the following four values.\n",
    "\n",
    "`observation` (**object**): an environment-specific object representing the observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "`reward` (**float**): The amount of reward returned as a result of taking the action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "`terminated` (**bool**): – whether a terminal state (as defined under the MDP of the task) is reached. In this case further step() calls could return undefined results.\n",
    "\n",
    "`truncated` (**bool**) – whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached.\n",
    "\n",
    "`done` (**boolean**): whether it’s time to `reset` the environment again. Most (but not all) tasks are divided up into well-defined episodes, and `done` being `True` indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "`info` (**dict**): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an `action` and the environment returns an `observation` and a `reward`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a243f-4bce-4677-a6c0-3934943e231f",
   "metadata": {},
   "source": [
    "<img src=\"20240603_Py_ReinforcementLearning5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47051a-9aa3-499a-b4ac-3cba4279acfa",
   "metadata": {},
   "source": [
    "The process gets started by calling `reset()`, which returns an initial `observation`. So a more proper way of writing the previous code would be to respect the `done` flag. The observation (state) of the environment is given by four parameters, describing the physical measurements: cart position, cart velocity, pole angle, and pole velocity (at tip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5838f14-87b5-40b1-a283-51e857f0189b",
   "metadata": {},
   "source": [
    "### Spaces\n",
    "\n",
    "In the above examples, we've been sampling random actions from the environment's action space. But what actually are those actions? Every environment comes with an `action_space` and an `observation_space`. These attributes are of type `Space`, and they describe the format of valid actions and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a2584d-1749-4d42-b9a3-916dcb00f30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action space\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af8690a-f88e-4665-a5ae-711db18dbd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state or observation space\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e3cf1-2ac7-4490-85aa-1624b512c5e7",
   "metadata": {},
   "source": [
    "The `Discrete` space allows a fixed range of non-negative numbers, so in this case valid `action`s are either 0 or 1. The `Box` space represents an $n$-dimensional box, so valid `observations` will be an array of 4 numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e78cfe-de72-47c4-b64a-1744b4f438f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "# check box bounds\n",
    "print(f'High: {env.observation_space.high}')\n",
    "print(f'Low: {env.observation_space.low}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f94b69-23fb-4248-971e-664cf3498cd9",
   "metadata": {},
   "source": [
    "## Learning Objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d8986ce-6537-4ce1-b726-09c22c98f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 1 | state=[-0.01339098  0.19733317 -0.01319055 -0.31613022] | action=1 | reward=1.0\n",
      "step= 2 | state=[-0.00944431  0.3926405  -0.01951315 -0.61294365] | action=1 | reward=1.0\n",
      "step= 3 | state=[-0.0015915   0.5880296  -0.03177202 -0.91170806] | action=1 | reward=1.0\n",
      "step= 4 | state=[ 0.01016909  0.7835667  -0.05000619 -1.214205  ] | action=1 | reward=1.0\n",
      "step= 5 | state=[ 0.02584042  0.5891244  -0.07429029 -0.93760186] | action=0 | reward=1.0\n",
      "step= 6 | state=[ 0.03762291  0.7851652  -0.09304233 -1.2526733 ] | action=1 | reward=1.0\n",
      "step= 7 | state=[ 0.05332622  0.9813477  -0.11809579 -1.5729891 ] | action=1 | reward=1.0\n",
      "step= 8 | state=[ 0.07295316  0.78781587 -0.14955558 -1.3193529 ] | action=0 | reward=1.0\n",
      "step= 9 | state=[ 0.08870948  0.9844783  -0.17594263 -1.6548593 ] | action=1 | reward=1.0\n",
      "step=10 | state=[ 0.10839905  1.1811632  -0.2090398  -1.9967927 ] | action=1 | reward=1.0\n",
      "step=11 | state=[ 0.13202232  1.3777697  -0.24897566 -2.3462934 ] | action=1 | reward=1.0\n",
      "*** FAILED ***\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for e in range(1, 200):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)                 # stepping forward one step \n",
    "    print(f'step={e:2d} | state={observation} | action={action} | reward={reward}')\n",
    "    if (terminated or truncated) and (e + 1) <= 200:                                    # failure if less than 200 steps\n",
    "        print('*** FAILED ***')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bb843-b7b7-4231-a6a3-a08289bfbf15",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6138a0a-b70e-49f9-aca8-1cb00412e8b1",
   "metadata": {},
   "source": [
    "[1] [OpenAI Spinning Up](https://urldefense.com/v3/__https://spinningup.openai.com/en/latest/index.html__;!!KGvANbslH1YjwA!9U83PW6hTtjaGWAdnbEhuGXT52hTBHA_7H7DaLF4x-Tt-t33K3c6LL27gG56cQAp7nYn2Cd3AYy-OWX0L_CBqt6KcZbRYFo7TiQ$)\n",
    "\n",
    "[2] [OpenAI Gym](https://urldefense.com/v3/__https://www.gymlibrary.dev/__;!!KGvANbslH1YjwA!9U83PW6hTtjaGWAdnbEhuGXT52hTBHA_7H7DaLF4x-Tt-t33K3c6LL27gG56cQAp7nYn2Cd3AYy-OWX0L_CBqt6KcZbRvQRpyGw$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3cde3-43d7-454c-9888-f35b331e3fc1",
   "metadata": {},
   "source": [
    "[Kannan Singaravelu.](https://www.linkedin.com/in/kannansi/) | [GitHub](https://urldefense.com/v3/__https://github.com/kannansingaravelu__;!!KGvANbslH1YjwA!9U83PW6hTtjaGWAdnbEhuGXT52hTBHA_7H7DaLF4x-Tt-t33K3c6LL27gG56cQAp7nYn2Cd3AYy-OWX0L_CBqt6KcZbR2-XAC4M$) | [LinkedIn](https://urldefense.com/v3/__https://www.linkedin.com/in/kannansi__;!!KGvANbslH1YjwA!9U83PW6hTtjaGWAdnbEhuGXT52hTBHA_7H7DaLF4x-Tt-t33K3c6LL27gG56cQAp7nYn2Cd3AYy-OWX0L_CBqt6KcZbRDD453dU$) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
