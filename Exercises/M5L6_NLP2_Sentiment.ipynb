{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment prediction using Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from tokenizer_class import FullTokenizer, is_number\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Load the pre-trained GloVe 300 dimensional embedding matrix\n",
    "Obtained here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'data/glove.6B.300d.txt'\n",
    "\n",
    "df_embedding = pd.read_csv(embedding_file, sep=' ', index_col=0,\n",
    "                           header=None, na_values=None,\n",
    "                           keep_default_na=False,\n",
    "                           quoting=csv.QUOTE_NONE)\n",
    "df_embedding.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>0.33048</td>\n",
       "      <td>0.116850</td>\n",
       "      <td>-0.411260</td>\n",
       "      <td>0.188480</td>\n",
       "      <td>0.321090</td>\n",
       "      <td>-0.813960</td>\n",
       "      <td>-0.051148</td>\n",
       "      <td>-0.337450</td>\n",
       "      <td>-0.204780</td>\n",
       "      <td>-1.198300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858400</td>\n",
       "      <td>0.252970</td>\n",
       "      <td>-0.539270</td>\n",
       "      <td>-0.595040</td>\n",
       "      <td>-0.248270</td>\n",
       "      <td>-1.113000</td>\n",
       "      <td>0.42782</td>\n",
       "      <td>-0.236000</td>\n",
       "      <td>0.375960</td>\n",
       "      <td>-0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hearted</th>\n",
       "      <td>-0.20698</td>\n",
       "      <td>-0.404830</td>\n",
       "      <td>-0.267510</td>\n",
       "      <td>-0.591330</td>\n",
       "      <td>-0.064752</td>\n",
       "      <td>0.043845</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>-0.105650</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041760</td>\n",
       "      <td>0.238870</td>\n",
       "      <td>-0.110440</td>\n",
       "      <td>0.238910</td>\n",
       "      <td>0.282370</td>\n",
       "      <td>-0.652840</td>\n",
       "      <td>-0.58310</td>\n",
       "      <td>0.072921</td>\n",
       "      <td>-0.067318</td>\n",
       "      <td>0.518940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dosage</th>\n",
       "      <td>-0.65587</td>\n",
       "      <td>0.827610</td>\n",
       "      <td>-0.055756</td>\n",
       "      <td>0.357710</td>\n",
       "      <td>0.394540</td>\n",
       "      <td>-0.125710</td>\n",
       "      <td>-0.314500</td>\n",
       "      <td>-0.197430</td>\n",
       "      <td>-0.250880</td>\n",
       "      <td>-0.710290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332710</td>\n",
       "      <td>0.171840</td>\n",
       "      <td>0.229170</td>\n",
       "      <td>-0.506400</td>\n",
       "      <td>0.223940</td>\n",
       "      <td>-0.711440</td>\n",
       "      <td>-0.51024</td>\n",
       "      <td>-0.298890</td>\n",
       "      <td>-0.090484</td>\n",
       "      <td>-0.322730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightyears</th>\n",
       "      <td>-0.22943</td>\n",
       "      <td>-0.682830</td>\n",
       "      <td>-0.280610</td>\n",
       "      <td>0.480470</td>\n",
       "      <td>-0.135190</td>\n",
       "      <td>-0.379870</td>\n",
       "      <td>-0.172250</td>\n",
       "      <td>-0.115170</td>\n",
       "      <td>0.552260</td>\n",
       "      <td>0.839840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395030</td>\n",
       "      <td>0.395180</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.878330</td>\n",
       "      <td>-0.37824</td>\n",
       "      <td>0.267460</td>\n",
       "      <td>-0.076411</td>\n",
       "      <td>-0.030604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ragni</th>\n",
       "      <td>-0.13872</td>\n",
       "      <td>0.161070</td>\n",
       "      <td>0.525340</td>\n",
       "      <td>0.042151</td>\n",
       "      <td>-0.436300</td>\n",
       "      <td>-0.484830</td>\n",
       "      <td>0.134350</td>\n",
       "      <td>0.033304</td>\n",
       "      <td>0.360310</td>\n",
       "      <td>0.768940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460640</td>\n",
       "      <td>0.338860</td>\n",
       "      <td>0.124590</td>\n",
       "      <td>-0.131970</td>\n",
       "      <td>-0.106400</td>\n",
       "      <td>-0.601260</td>\n",
       "      <td>0.25891</td>\n",
       "      <td>1.098600</td>\n",
       "      <td>0.432760</td>\n",
       "      <td>-0.302610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poshteh-ye</th>\n",
       "      <td>-0.56235</td>\n",
       "      <td>0.084654</td>\n",
       "      <td>-0.066012</td>\n",
       "      <td>0.231860</td>\n",
       "      <td>-0.147000</td>\n",
       "      <td>-0.511670</td>\n",
       "      <td>0.572680</td>\n",
       "      <td>0.350820</td>\n",
       "      <td>-0.723060</td>\n",
       "      <td>1.059200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047565</td>\n",
       "      <td>0.557360</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>0.371090</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>-0.045595</td>\n",
       "      <td>-0.53871</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.279560</td>\n",
       "      <td>-0.607250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mdivani</th>\n",
       "      <td>0.36266</td>\n",
       "      <td>-0.045557</td>\n",
       "      <td>-0.034230</td>\n",
       "      <td>0.157770</td>\n",
       "      <td>-0.653320</td>\n",
       "      <td>-0.478830</td>\n",
       "      <td>0.234880</td>\n",
       "      <td>-0.129390</td>\n",
       "      <td>0.160240</td>\n",
       "      <td>0.981830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099246</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>0.445630</td>\n",
       "      <td>0.029854</td>\n",
       "      <td>0.237290</td>\n",
       "      <td>-0.553470</td>\n",
       "      <td>0.21815</td>\n",
       "      <td>0.682050</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>-0.342590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harriers</th>\n",
       "      <td>0.38212</td>\n",
       "      <td>0.665550</td>\n",
       "      <td>0.366030</td>\n",
       "      <td>0.118270</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.104760</td>\n",
       "      <td>0.538450</td>\n",
       "      <td>0.458350</td>\n",
       "      <td>0.065454</td>\n",
       "      <td>0.874270</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.532600</td>\n",
       "      <td>0.975060</td>\n",
       "      <td>-0.196270</td>\n",
       "      <td>-0.611260</td>\n",
       "      <td>-0.785580</td>\n",
       "      <td>-0.455280</td>\n",
       "      <td>0.37727</td>\n",
       "      <td>-0.640130</td>\n",
       "      <td>-0.900120</td>\n",
       "      <td>0.579210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kolah</th>\n",
       "      <td>-0.25497</td>\n",
       "      <td>0.078224</td>\n",
       "      <td>-0.314990</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>0.041182</td>\n",
       "      <td>-0.240370</td>\n",
       "      <td>-0.041194</td>\n",
       "      <td>0.519860</td>\n",
       "      <td>-0.778400</td>\n",
       "      <td>0.574290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341470</td>\n",
       "      <td>0.626570</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>-0.449710</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>-1.003600</td>\n",
       "      <td>-0.15172</td>\n",
       "      <td>0.197940</td>\n",
       "      <td>-0.014791</td>\n",
       "      <td>0.423450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11-week</th>\n",
       "      <td>0.25130</td>\n",
       "      <td>0.842190</td>\n",
       "      <td>0.396610</td>\n",
       "      <td>0.275760</td>\n",
       "      <td>0.087887</td>\n",
       "      <td>0.737490</td>\n",
       "      <td>0.125450</td>\n",
       "      <td>-0.847930</td>\n",
       "      <td>0.401530</td>\n",
       "      <td>0.813140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183810</td>\n",
       "      <td>0.695540</td>\n",
       "      <td>-0.114280</td>\n",
       "      <td>0.336450</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>0.361470</td>\n",
       "      <td>0.43794</td>\n",
       "      <td>-0.433570</td>\n",
       "      <td>-0.540260</td>\n",
       "      <td>0.424180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1         2         3         4         5         6    \\\n",
       "0                                                                       \n",
       "sonar       0.33048  0.116850 -0.411260  0.188480  0.321090 -0.813960   \n",
       "hearted    -0.20698 -0.404830 -0.267510 -0.591330 -0.064752  0.043845   \n",
       "dosage     -0.65587  0.827610 -0.055756  0.357710  0.394540 -0.125710   \n",
       "lightyears -0.22943 -0.682830 -0.280610  0.480470 -0.135190 -0.379870   \n",
       "ragni      -0.13872  0.161070  0.525340  0.042151 -0.436300 -0.484830   \n",
       "poshteh-ye -0.56235  0.084654 -0.066012  0.231860 -0.147000 -0.511670   \n",
       "mdivani     0.36266 -0.045557 -0.034230  0.157770 -0.653320 -0.478830   \n",
       "harriers    0.38212  0.665550  0.366030  0.118270  0.145510  0.104760   \n",
       "kolah      -0.25497  0.078224 -0.314990 -0.011261  0.041182 -0.240370   \n",
       "11-week     0.25130  0.842190  0.396610  0.275760  0.087887  0.737490   \n",
       "\n",
       "                 7         8         9         10   ...       291       292  \\\n",
       "0                                                   ...                       \n",
       "sonar      -0.051148 -0.337450 -0.204780 -1.198300  ... -0.858400  0.252970   \n",
       "hearted     0.084845 -0.105650  0.011034 -0.076297  ... -0.041760  0.238870   \n",
       "dosage     -0.314500 -0.197430 -0.250880 -0.710290  ... -0.332710  0.171840   \n",
       "lightyears -0.172250 -0.115170  0.552260  0.839840  ...  0.395030  0.395180   \n",
       "ragni       0.134350  0.033304  0.360310  0.768940  ...  0.460640  0.338860   \n",
       "poshteh-ye  0.572680  0.350820 -0.723060  1.059200  ...  0.047565  0.557360   \n",
       "mdivani     0.234880 -0.129390  0.160240  0.981830  ... -0.099246  0.012156   \n",
       "harriers    0.538450  0.458350  0.065454  0.874270  ... -1.532600  0.975060   \n",
       "kolah      -0.041194  0.519860 -0.778400  0.574290  ... -0.341470  0.626570   \n",
       "11-week     0.125450 -0.847930  0.401530  0.813140  ...  0.183810  0.695540   \n",
       "\n",
       "                 293       294       295       296      297       298  \\\n",
       "0                                                                       \n",
       "sonar      -0.539270 -0.595040 -0.248270 -1.113000  0.42782 -0.236000   \n",
       "hearted    -0.110440  0.238910  0.282370 -0.652840 -0.58310  0.072921   \n",
       "dosage      0.229170 -0.506400  0.223940 -0.711440 -0.51024 -0.298890   \n",
       "lightyears  0.264700 -0.000818  0.009533 -0.878330 -0.37824  0.267460   \n",
       "ragni       0.124590 -0.131970 -0.106400 -0.601260  0.25891  1.098600   \n",
       "poshteh-ye  0.006690  0.371090  0.268260 -0.045595 -0.53871  0.076358   \n",
       "mdivani     0.445630  0.029854  0.237290 -0.553470  0.21815  0.682050   \n",
       "harriers   -0.196270 -0.611260 -0.785580 -0.455280  0.37727 -0.640130   \n",
       "kolah       0.012646 -0.449710  0.166700 -1.003600 -0.15172  0.197940   \n",
       "11-week    -0.114280  0.336450  0.027179  0.361470  0.43794 -0.433570   \n",
       "\n",
       "                 299       300  \n",
       "0                               \n",
       "sonar       0.375960 -0.427000  \n",
       "hearted    -0.067318  0.518940  \n",
       "dosage     -0.090484 -0.322730  \n",
       "lightyears -0.076411 -0.030604  \n",
       "ragni       0.432760 -0.302610  \n",
       "poshteh-ye  0.279560 -0.607250  \n",
       "mdivani     0.038624 -0.342590  \n",
       "harriers   -0.900120  0.579210  \n",
       "kolah      -0.014791  0.423450  \n",
       "11-week    -0.540260  0.424180  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "$\\Large \\text{sim(U, V)} = \\frac {U^T\\cdot{V}} {||U||_2 ||V||_2} = cos(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cosine_sim.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    cs = np.dot(u, v) / (np.linalg.norm(u, ord=2) * np.linalg.norm(v, ord=2))\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5643459466769057"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(df_embedding.loc['france'], df_embedding.loc['italy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025038104793071447"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(df_embedding.loc['ball'], df_embedding.loc['crocodile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6228158930510513"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(df_embedding.loc['france'] - df_embedding.loc['paris'],\n",
    "                  df_embedding.loc['rome'] - df_embedding.loc['italy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/google-10000-english.txt', 'r') as f:\n",
    "    vocab = f.read().splitlines()\n",
    "df_embedding_small = df_embedding.reindex(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(a, b, c):\n",
    "    emb_a, emb_b, emb_c = df_embedding_small.loc[a], \\\n",
    "                          df_embedding_small.loc[b], \\\n",
    "                          df_embedding_small.loc[c]\n",
    "    max_sim = -100\n",
    "    result = None\n",
    "    for w, emb_w in df_embedding_small.iterrows():\n",
    "        if w in [a, b, c]:\n",
    "            continue\n",
    "        sim = cosine_similarity(emb_b - emb_a, emb_w - emb_c)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            result = w\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris -> france : london -> britain\n",
      "paris -> france : rome -> italy\n",
      "paris -> france : madrid -> spain\n",
      "paris -> france : berlin -> germany\n",
      "man -> king : woman -> queen\n",
      "walk -> walked : go -> went\n",
      "man -> woman : boy -> girl\n",
      "apple -> juice : mother -> lemon\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ('paris', 'france', 'london'),\n",
    "    ('paris', 'france', 'rome'),\n",
    "    ('paris', 'france', 'madrid'),\n",
    "    ('paris', 'france', 'berlin'),\n",
    "    ('man', 'king', 'woman'),\n",
    "    ('walk', 'walked', 'go'),\n",
    "    ('man', 'woman', 'boy'),\n",
    "    ('apple', 'juice', 'mother')\n",
    "]\n",
    "\n",
    "for t in test_cases:\n",
    "    r = word_analogy(t[0], t[1], t[2])\n",
    "    print('{} -> {} : {} -> {}'.format(t[0], t[1], t[2], r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('data/news_data.csv',\n",
    "                        usecols = ['TEXT', 'SENTIMENT'])\n",
    "news_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611430</th>\n",
       "      <td>0.94</td>\n",
       "      <td>Commonwealth Bank of Australia Upgraded to Buy from Hold by Bell Potter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611431</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>Qbe Insurance Downgraded to Hold from Buy by Bell Potter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611432</th>\n",
       "      <td>0.49</td>\n",
       "      <td>Collective &amp; Roland Team for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611433</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>Mcmillan Shakespeare Downgraded to Neutral from Outperform by Macquarie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611434</th>\n",
       "      <td>0.95</td>\n",
       "      <td>Nearmap Initiated at Outperform by Macquarie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SENTIMENT  \\\n",
       "611430       0.94   \n",
       "611431      -0.58   \n",
       "611432       0.49   \n",
       "611433      -0.58   \n",
       "611434       0.95   \n",
       "\n",
       "                                                                           TEXT  \n",
       "611430  Commonwealth Bank of Australia Upgraded to Buy from Hold by Bell Potter  \n",
       "611431                 Qbe Insurance Downgraded to Hold from Buy by Bell Potter  \n",
       "611432                                             Collective & Roland Team for  \n",
       "611433  Mcmillan Shakespeare Downgraded to Neutral from Outperform by Macquarie  \n",
       "611434                             Nearmap Initiated at Outperform by Macquarie  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizer and compress the embedding matrix\n",
    "\n",
    "Build out a set of all tokens from every `TEXT` record using the full tokenizer (with its vocab initialised from the embedding matrix), then filter the embedding matrix dictionary to only include those tokens that exist in the `TEXT`. Then, re-instantiate the tokenizer using the embedding matrix dictionary. This speeds up training for testing/POC purpose by minimizing the size of the embedding tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved tokenizer?\n",
    "if True:\n",
    "    with open('model/tokenizer.pkl', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    df_embedding = df_embedding.reindex(tokenizer.vocab.keys())\n",
    "    df_embedding.sort_index(inplace=True)\n",
    "    df_embedding.loc['<START>'] = np.zeros(df_embedding.shape[1])\n",
    "    df_embedding.loc['<UNK>'] = df_embedding.mean()\n",
    "    df_embedding.loc['<NUMBER>'] = np.ones(df_embedding.shape[1])/2\n",
    "    df_embedding.loc['<PAD>'] = np.ones(df_embedding.shape[1])\n",
    "    \n",
    "else:\n",
    "    tokenizer = FullTokenizer(df_embedding.index.values)\n",
    "\n",
    "    def process(arr):\n",
    "        for i in map(tokenizer.tokenize, arr):\n",
    "            tokens.update(i)\n",
    "        return tokens\n",
    "\n",
    "    tokens = set()\n",
    "    if __name__ == '__main__':\n",
    "        p = mp.Pool(processes=mp.cpu_count())\n",
    "        split_arr = np.array_split(news_data.TEXT.values, mp.cpu_count())\n",
    "        pool_results = p.map(process, split_arr)\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "        all_tokens = set.union(*pool_results)\n",
    "    \n",
    "    all_tokens = set([t for t in all_tokens if not is_number(t)])\n",
    "    df_embedding = df_embedding.reindex(all_tokens).dropna()\n",
    "    df_embedding.sort_index(inplace=True)\n",
    "    df_embedding.loc['<START>'] = np.zeros(df_embedding.shape[1])\n",
    "    df_embedding.loc['<UNK>'] = df_embedding.mean()\n",
    "    df_embedding.loc['<NUMBER>'] = np.ones(df_embedding.shape[1])/2\n",
    "    df_embedding.loc['<PAD>'] = np.ones(df_embedding.shape[1])\n",
    "\n",
    "tokenizer = FullTokenizer(df_embedding.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer',\n",
       " 'takes',\n",
       " 'a',\n",
       " 'string',\n",
       " 'and',\n",
       " 'breaks',\n",
       " 'it',\n",
       " 'down',\n",
       " 'into',\n",
       " 'tokens',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'Tokenizer takes a string and breaks it down into tokens.'\n",
    "tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1241, 56549, 1252, 55330, 3529, 8641, 29211, 17049, 28807, 58228, 19]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise tokens in the word embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.056789</td>\n",
       "      <td>-0.022786</td>\n",
       "      <td>-0.002645</td>\n",
       "      <td>0.069256</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>-0.036345</td>\n",
       "      <td>0.032409</td>\n",
       "      <td>-0.064922</td>\n",
       "      <td>-0.041482</td>\n",
       "      <td>0.234542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020075</td>\n",
       "      <td>0.102891</td>\n",
       "      <td>0.090614</td>\n",
       "      <td>-0.024519</td>\n",
       "      <td>-0.070710</td>\n",
       "      <td>-0.191057</td>\n",
       "      <td>0.048234</td>\n",
       "      <td>0.136562</td>\n",
       "      <td>0.088450</td>\n",
       "      <td>-0.031001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>takes</th>\n",
       "      <td>-0.265480</td>\n",
       "      <td>-0.109470</td>\n",
       "      <td>0.179890</td>\n",
       "      <td>-0.077156</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.027737</td>\n",
       "      <td>-0.180900</td>\n",
       "      <td>-0.145750</td>\n",
       "      <td>0.341820</td>\n",
       "      <td>-1.314900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278790</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>-0.213730</td>\n",
       "      <td>-0.148400</td>\n",
       "      <td>-0.123810</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>-0.331660</td>\n",
       "      <td>0.319620</td>\n",
       "      <td>-0.323520</td>\n",
       "      <td>0.002213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.297120</td>\n",
       "      <td>0.094049</td>\n",
       "      <td>-0.096662</td>\n",
       "      <td>-0.344000</td>\n",
       "      <td>-0.184830</td>\n",
       "      <td>-0.123290</td>\n",
       "      <td>-0.116560</td>\n",
       "      <td>-0.099692</td>\n",
       "      <td>0.172650</td>\n",
       "      <td>-1.638600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075972</td>\n",
       "      <td>-0.424260</td>\n",
       "      <td>-0.396700</td>\n",
       "      <td>0.326830</td>\n",
       "      <td>0.620490</td>\n",
       "      <td>0.347190</td>\n",
       "      <td>0.269520</td>\n",
       "      <td>0.059717</td>\n",
       "      <td>-0.228530</td>\n",
       "      <td>0.296020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>string</th>\n",
       "      <td>0.490790</td>\n",
       "      <td>0.373550</td>\n",
       "      <td>-0.108000</td>\n",
       "      <td>-0.181940</td>\n",
       "      <td>-0.406680</td>\n",
       "      <td>-0.214930</td>\n",
       "      <td>-0.200550</td>\n",
       "      <td>-0.469240</td>\n",
       "      <td>-0.120310</td>\n",
       "      <td>-0.924760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161770</td>\n",
       "      <td>-0.332490</td>\n",
       "      <td>-0.319580</td>\n",
       "      <td>0.347060</td>\n",
       "      <td>0.116960</td>\n",
       "      <td>0.870680</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>-0.543940</td>\n",
       "      <td>-0.368430</td>\n",
       "      <td>-0.225760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.038466</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>0.082747</td>\n",
       "      <td>-0.389230</td>\n",
       "      <td>-0.214310</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>-0.025657</td>\n",
       "      <td>0.095780</td>\n",
       "      <td>0.238600</td>\n",
       "      <td>-1.634200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045194</td>\n",
       "      <td>-0.204050</td>\n",
       "      <td>-0.210970</td>\n",
       "      <td>-0.110250</td>\n",
       "      <td>0.021766</td>\n",
       "      <td>0.441290</td>\n",
       "      <td>0.327970</td>\n",
       "      <td>-0.334270</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.059703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breaks</th>\n",
       "      <td>-0.273560</td>\n",
       "      <td>0.012609</td>\n",
       "      <td>-0.441310</td>\n",
       "      <td>0.410770</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.144790</td>\n",
       "      <td>0.190910</td>\n",
       "      <td>-0.257050</td>\n",
       "      <td>0.442460</td>\n",
       "      <td>-0.934800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304630</td>\n",
       "      <td>0.051168</td>\n",
       "      <td>-0.794090</td>\n",
       "      <td>0.129680</td>\n",
       "      <td>-0.291050</td>\n",
       "      <td>-0.219350</td>\n",
       "      <td>-0.609280</td>\n",
       "      <td>-0.054624</td>\n",
       "      <td>0.389920</td>\n",
       "      <td>0.752370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.033284</td>\n",
       "      <td>-0.040754</td>\n",
       "      <td>-0.048377</td>\n",
       "      <td>0.120170</td>\n",
       "      <td>-0.139150</td>\n",
       "      <td>-0.176940</td>\n",
       "      <td>-0.062908</td>\n",
       "      <td>0.170560</td>\n",
       "      <td>0.200770</td>\n",
       "      <td>-2.428700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091222</td>\n",
       "      <td>-0.402000</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.230990</td>\n",
       "      <td>0.086138</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>0.065196</td>\n",
       "      <td>-0.154080</td>\n",
       "      <td>0.178060</td>\n",
       "      <td>-0.196830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>-0.081429</td>\n",
       "      <td>-0.110040</td>\n",
       "      <td>-0.031034</td>\n",
       "      <td>0.604570</td>\n",
       "      <td>0.067606</td>\n",
       "      <td>-0.316090</td>\n",
       "      <td>-0.460590</td>\n",
       "      <td>-0.202730</td>\n",
       "      <td>0.468520</td>\n",
       "      <td>-1.600900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212930</td>\n",
       "      <td>-0.172900</td>\n",
       "      <td>0.013380</td>\n",
       "      <td>0.450640</td>\n",
       "      <td>0.124720</td>\n",
       "      <td>0.549970</td>\n",
       "      <td>-0.196470</td>\n",
       "      <td>-0.390020</td>\n",
       "      <td>-0.062974</td>\n",
       "      <td>0.096852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>into</th>\n",
       "      <td>-0.182340</td>\n",
       "      <td>-0.192090</td>\n",
       "      <td>0.151670</td>\n",
       "      <td>-0.048763</td>\n",
       "      <td>0.283540</td>\n",
       "      <td>-0.025568</td>\n",
       "      <td>-0.398820</td>\n",
       "      <td>-0.014072</td>\n",
       "      <td>0.410290</td>\n",
       "      <td>-1.987000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058528</td>\n",
       "      <td>-0.133670</td>\n",
       "      <td>0.159020</td>\n",
       "      <td>0.072976</td>\n",
       "      <td>0.779430</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.073351</td>\n",
       "      <td>-0.248390</td>\n",
       "      <td>0.144680</td>\n",
       "      <td>0.176830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>-0.671420</td>\n",
       "      <td>-0.013502</td>\n",
       "      <td>0.078301</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>-0.039577</td>\n",
       "      <td>0.361740</td>\n",
       "      <td>0.370120</td>\n",
       "      <td>0.124180</td>\n",
       "      <td>0.449070</td>\n",
       "      <td>0.137810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519930</td>\n",
       "      <td>-0.119760</td>\n",
       "      <td>-0.075887</td>\n",
       "      <td>0.342280</td>\n",
       "      <td>0.248760</td>\n",
       "      <td>-0.271560</td>\n",
       "      <td>-0.126900</td>\n",
       "      <td>-0.202670</td>\n",
       "      <td>0.234910</td>\n",
       "      <td>-0.908890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.125590</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>-0.101230</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>0.136270</td>\n",
       "      <td>-0.107210</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>-1.678500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060148</td>\n",
       "      <td>-0.156190</td>\n",
       "      <td>-0.119490</td>\n",
       "      <td>0.234450</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>0.246180</td>\n",
       "      <td>-0.152420</td>\n",
       "      <td>-0.342240</td>\n",
       "      <td>-0.022394</td>\n",
       "      <td>0.136840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7    \\\n",
       "0                                                                              \n",
       "<UNK>   0.056789 -0.022786 -0.002645  0.069256  0.010156 -0.036345  0.032409   \n",
       "takes  -0.265480 -0.109470  0.179890 -0.077156 -0.001871 -0.027737 -0.180900   \n",
       "a      -0.297120  0.094049 -0.096662 -0.344000 -0.184830 -0.123290 -0.116560   \n",
       "string  0.490790  0.373550 -0.108000 -0.181940 -0.406680 -0.214930 -0.200550   \n",
       "and     0.038466 -0.039792  0.082747 -0.389230 -0.214310  0.170200 -0.025657   \n",
       "breaks -0.273560  0.012609 -0.441310  0.410770  0.134850  0.144790  0.190910   \n",
       "it      0.033284 -0.040754 -0.048377  0.120170 -0.139150 -0.176940 -0.062908   \n",
       "down   -0.081429 -0.110040 -0.031034  0.604570  0.067606 -0.316090 -0.460590   \n",
       "into   -0.182340 -0.192090  0.151670 -0.048763  0.283540 -0.025568 -0.398820   \n",
       "tokens -0.671420 -0.013502  0.078301  0.009528 -0.039577  0.361740  0.370120   \n",
       ".      -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
       "\n",
       "             8         9         10   ...       291       292       293  \\\n",
       "0                                     ...                                 \n",
       "<UNK>  -0.064922 -0.041482  0.234542  ...  0.020075  0.102891  0.090614   \n",
       "takes  -0.145750  0.341820 -1.314900  ... -0.278790  0.128100 -0.213730   \n",
       "a      -0.099692  0.172650 -1.638600  ...  0.075972 -0.424260 -0.396700   \n",
       "string -0.469240 -0.120310 -0.924760  ...  0.161770 -0.332490 -0.319580   \n",
       "and     0.095780  0.238600 -1.634200  ...  0.045194 -0.204050 -0.210970   \n",
       "breaks -0.257050  0.442460 -0.934800  ... -0.304630  0.051168 -0.794090   \n",
       "it      0.170560  0.200770 -2.428700  ...  0.091222 -0.402000  0.154300   \n",
       "down   -0.202730  0.468520 -1.600900  ...  0.212930 -0.172900  0.013380   \n",
       "into   -0.014072  0.410290 -1.987000  ... -0.058528 -0.133670  0.159020   \n",
       "tokens  0.124180  0.449070  0.137810  ... -0.519930 -0.119760 -0.075887   \n",
       ".       0.236970  0.328700 -1.678500  ...  0.060148 -0.156190 -0.119490   \n",
       "\n",
       "             294       295       296       297       298       299       300  \n",
       "0                                                                             \n",
       "<UNK>  -0.024519 -0.070710 -0.191057  0.048234  0.136562  0.088450 -0.031001  \n",
       "takes  -0.148400 -0.123810  0.011745 -0.331660  0.319620 -0.323520  0.002213  \n",
       "a       0.326830  0.620490  0.347190  0.269520  0.059717 -0.228530  0.296020  \n",
       "string  0.347060  0.116960  0.870680  0.264700 -0.543940 -0.368430 -0.225760  \n",
       "and    -0.110250  0.021766  0.441290  0.327970 -0.334270  0.011807  0.059703  \n",
       "breaks  0.129680 -0.291050 -0.219350 -0.609280 -0.054624  0.389920  0.752370  \n",
       "it      0.230990  0.086138 -0.002428  0.065196 -0.154080  0.178060 -0.196830  \n",
       "down    0.450640  0.124720  0.549970 -0.196470 -0.390020 -0.062974  0.096852  \n",
       "into    0.072976  0.779430  0.792400  0.073351 -0.248390  0.144680  0.176830  \n",
       "tokens  0.342280  0.248760 -0.271560 -0.126900 -0.202670  0.234910 -0.908890  \n",
       ".       0.234450  0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
       "\n",
       "[11 rows x 300 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding.iloc[tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentence))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define encoding/decoding methods\n",
    "The `encode` method takes a string and converts it to IDs using the tokenizer dictionary. `decode` performs the opposite action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(string):\n",
    "    encoding = [\"<START>\"] + tokenizer.tokenize(string)\n",
    "    encoding = tokenizer.convert_tokens_to_ids(encoding)\n",
    "    return encoding\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the dataset\n",
    "We encode `TEXT` using the tokenizer to obtain IDs that correspond to each token's index within the embedding matrix. We use multiprocessing to process large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    res = df.TEXT.apply(encode)\n",
    "    return res\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = mp.Pool(processes=mp.cpu_count())\n",
    "    split_dfs = np.array_split(news_data, mp.cpu_count())\n",
    "    pool_results = p.map(process, split_dfs)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    # merging parts processed by different processes\n",
    "    dataset = pd.concat(pool_results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611430    [1240, 12768, 5885, 41528, 5144, 60208, 58166, 9550, 21961, 26614, 9589, 6649, 45148]\n",
       "611431                [1240, 46376, 28611, 17057, 58166, 26614, 21961, 9550, 9589, 6649, 45148]\n",
       "611432                                                    [1240, 12567, 5, 49267, 56981, 21423]\n",
       "611433                     [1240, 36652, 52076, 17057, 58166, 40196, 21961, 42449, 9589, 35053]\n",
       "611434                                            [1240, 1241, 28416, 4837, 42449, 9589, 35053]\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611430    [<START>, commonwealth, bank, of, australia, upgraded, to, buy, from, hold, by, bell, potter]\n",
       "611431                     [<START>, qbe, insurance, downgraded, to, hold, from, buy, by, bell, potter]\n",
       "611432                                                      [<START>, collective, &, roland, team, for]\n",
       "611433       [<START>, mcmillan, shakespeare, downgraded, to, neutral, from, outperform, by, macquarie]\n",
       "611434                                       [<START>, <UNK>, initiated, at, outperform, by, macquarie]\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail().apply(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad all sequences to maxlen (inferred from the dataset) to obtain equal length vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = dataset.apply(len).max() + 10\n",
    "data = keras.preprocessing.sequence.pad_sequences(dataset.values,\n",
    "                                           value=tokenizer.vocab[\"<PAD>\"],\n",
    "                                           padding='post',\n",
    "                                           maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "<img src=\"img/lstm_diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 58, 300)           19349400  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 58, 128)           219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 58, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 19,700,761\n",
      "Trainable params: 351,361\n",
      "Non-trainable params: 19,349,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neurons = 128\n",
    "dropout = 0.40\n",
    "\n",
    "my_embedding = layers.Embedding(input_dim=df_embedding.shape[0],\n",
    "                                output_dim=df_embedding.shape[1],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False)\n",
    "\n",
    "my_embedding.build((None, ))\n",
    "my_embedding.set_weights([df_embedding.values])\n",
    "\n",
    "model = keras.Sequential([\n",
    "    my_embedding,\n",
    "    layers.LSTM(neurons, return_sequences=True),\n",
    "    layers.Dropout(dropout),\n",
    "    layers.LSTM(neurons, return_sequences=False),\n",
    "    layers.Dropout(dropout),\n",
    "    layers.Dense(1, activation='tanh')\n",
    "])\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res = K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                    optimizer=optimizer,\n",
    "              metrics=[r2_keras])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 502,692\n",
      "test: 55,854\n"
     ]
    }
   ],
   "source": [
    "train_pct = 0.90\n",
    "\n",
    "y = news_data.SENTIMENT.values\n",
    "y_train, y_test = y[:int(data.shape[0] * train_pct)], \\\n",
    "                  y[-int(data.shape[0] * (1-train_pct)):]\n",
    "\n",
    "X_train, X_test = data[:int(data.shape[0] * train_pct)], \\\n",
    "                  data[-int(data.shape[0] * (1-train_pct)):], \\\n",
    "\n",
    "print('train: {:,}\\ntest: {:,}'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 502692 samples, validate on 55854 samples\n",
      "Epoch 1/10\n",
      " 23552/502692 [>.............................] - ETA: 10:58 - loss: 0.3332 - r2_keras: -0.4449"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ec55e10505d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          epochs = 10,\n",
    "          batch_size = 1024,\n",
    "          shuffle=True,\n",
    "          validation_split=0.2,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 502692 samples, validate on 55854 samples\n",
      "Epoch 1/10\n",
      "502692/502692 [==============================] - 69s 138us/sample - loss: 0.2349 - r2_keras: -0.0109 - val_loss: 0.1081 - val_r2_keras: 0.5329\n",
      "Epoch 2/10\n",
      "502692/502692 [==============================] - 64s 127us/sample - loss: 0.0691 - r2_keras: 0.7025 - val_loss: 0.0506 - val_r2_keras: 0.7817\n",
      "Epoch 3/10\n",
      "502692/502692 [==============================] - 63s 126us/sample - loss: 0.0571 - r2_keras: 0.7542 - val_loss: 0.0454 - val_r2_keras: 0.8041\n",
      "Epoch 4/10\n",
      "502692/502692 [==============================] - 64s 128us/sample - loss: 0.0541 - r2_keras: 0.7671 - val_loss: 0.0443 - val_r2_keras: 0.8088\n",
      "Epoch 5/10\n",
      "502692/502692 [==============================] - 65s 129us/sample - loss: 0.0524 - r2_keras: 0.7744 - val_loss: 0.0426 - val_r2_keras: 0.8157\n",
      "Epoch 6/10\n",
      "502692/502692 [==============================] - 63s 126us/sample - loss: 0.0510 - r2_keras: 0.7804 - val_loss: 0.0424 - val_r2_keras: 0.8173\n",
      "Epoch 7/10\n",
      "502692/502692 [==============================] - 63s 126us/sample - loss: 0.0500 - r2_keras: 0.7848 - val_loss: 0.0407 - val_r2_keras: 0.8244\n",
      "Epoch 8/10\n",
      "502692/502692 [==============================] - 64s 128us/sample - loss: 0.0491 - r2_keras: 0.7885 - val_loss: 0.0404 - val_r2_keras: 0.8254\n",
      "Epoch 9/10\n",
      "502692/502692 [==============================] - 63s 126us/sample - loss: 0.0489 - r2_keras: 0.7894 - val_loss: 0.0401 - val_r2_keras: 0.8270\n",
      "Epoch 10/10\n",
      "502692/502692 [==============================] - 65s 128us/sample - loss: 0.0479 - r2_keras: 0.7935 - val_loss: 0.0397 - val_r2_keras: 0.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f02a16dd0f0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          epochs = 10,\n",
    "          batch_size = 1024,\n",
    "          shuffle=True,\n",
    "          validation_split=0.2,\n",
    "          validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model/tf-lstm-model.h5')\n",
    "    \n",
    "# with open('rp_model/tokenizer.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/tf-lstm-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a simple helper function to format text data into machine-readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(txt):\n",
    "    if isinstance(txt, str):\n",
    "        txt = keras.preprocessing.sequence.pad_sequences(\n",
    "            [encode(txt)],\n",
    "            value=tokenizer.vocab[\"<PAD>\"],\n",
    "            padding='post',\n",
    "            maxlen=maxlen)\n",
    "    return model.predict(txt)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.54968196"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('CEO found guilty of fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6829254"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('IBM misses earnings expectations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93568856"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''Morgan Stanley analyst Andrew Berens initiates coverage\n",
    "           on Blueprint Medicines (NASDAQ:BPMC) with a Overweight''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9526496"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''Senex Energy Upgraded to Outperform from Neutral by Credit Suisse''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.58416826"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''Qbe Insurance Downgraded to Hold from Buy by Bell Potter''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Topic Prediction model: Keras functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "<img src=\"img/lstm_bi_2o_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('data/news_data.csv',\n",
    "                        usecols = ['TEXT', 'SENTIMENT', 'TOPIC'])\n",
    "news_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>TOPIC</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611430</th>\n",
       "      <td>0.94</td>\n",
       "      <td>analyst-ratings-change</td>\n",
       "      <td>Commonwealth Bank of Australia Upgraded to Buy from Hold by Bell Potter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611431</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>analyst-ratings-change</td>\n",
       "      <td>Qbe Insurance Downgraded to Hold from Buy by Bell Potter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611432</th>\n",
       "      <td>0.49</td>\n",
       "      <td>partnership</td>\n",
       "      <td>Collective &amp; Roland Team for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611433</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>analyst-ratings-change</td>\n",
       "      <td>Mcmillan Shakespeare Downgraded to Neutral from Outperform by Macquarie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611434</th>\n",
       "      <td>0.95</td>\n",
       "      <td>analyst-ratings-set</td>\n",
       "      <td>Nearmap Initiated at Outperform by Macquarie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SENTIMENT                   TOPIC  \\\n",
       "611430       0.94  analyst-ratings-change   \n",
       "611431      -0.58  analyst-ratings-change   \n",
       "611432       0.49             partnership   \n",
       "611433      -0.58  analyst-ratings-change   \n",
       "611434       0.95     analyst-ratings-set   \n",
       "\n",
       "                                                                           TEXT  \n",
       "611430  Commonwealth Bank of Australia Upgraded to Buy from Hold by Bell Potter  \n",
       "611431                 Qbe Insurance Downgraded to Hold from Buy by Bell Potter  \n",
       "611432                                             Collective & Roland Team for  \n",
       "611433  Mcmillan Shakespeare Downgraded to Neutral from Outperform by Macquarie  \n",
       "611434                             Nearmap Initiated at Outperform by Macquarie  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# topics: 220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder_topic = preprocessing.LabelBinarizer()\n",
    "encoder_topic.fit(news_data.TOPIC.values)\n",
    "print('# topics: {}\\n'.format(len(encoder_topic.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"topic_sent_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence_input (InputLayer)     [(None, 58)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 58, 300)      19349400    sequence_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 58, 256)      439296      embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          197120      bi_lstm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "topic (Dense)                   (None, 220)          28380       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sentiment (Dense)               (None, 1)            129         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 20,014,325\n",
      "Trainable params: 664,925\n",
      "Non-trainable params: 19,349,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neurons = 128\n",
    "dropout = 0.20\n",
    "topic_output_length = encoder_topic.classes_.shape[0]\n",
    "\n",
    "my_embedding = layers.Embedding(input_dim=df_embedding.shape[0],\n",
    "                                output_dim=df_embedding.shape[1],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False,\n",
    "                                name='embedding_layer'\n",
    "                               )\n",
    "my_embedding.build((None, ))\n",
    "my_embedding.set_weights([df_embedding.values])\n",
    "\n",
    "sequence_input = keras.layers.Input((maxlen, ), name='sequence_input')\n",
    "\n",
    "X = my_embedding(sequence_input)\n",
    "X = layers.Bidirectional(\n",
    "    layers.LSTM(units=neurons,\n",
    "                return_sequences=True,\n",
    "                dropout=dropout),\n",
    "    name='bi_lstm_1')(X)\n",
    "X = layers.LSTM(units=neurons,\n",
    "                return_sequences=False,\n",
    "                dropout=dropout,\n",
    "                name='lstm_1')(X)\n",
    "\n",
    "# Two output layers: softmax for Topic and 1D tanh for sentiment\n",
    "topic_output = layers.Dense(topic_output_length,\n",
    "                            activation='softmax',\n",
    "                            name='topic')(X)\n",
    "sentiment = layers.Dense(units=1,\n",
    "                         activation='tanh',\n",
    "                         name='sentiment')(X)\n",
    "\n",
    "model = keras.models.Model(inputs=[sequence_input],\n",
    "                           outputs=[topic_output, sentiment],\n",
    "                           name='topic_sent_model')\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "model.compile(loss={'topic':'categorical_crossentropy',\n",
    "                    'sentiment':'mean_squared_error'},\n",
    "                    optimizer=optimizer,\n",
    "              metrics={'topic':'accuracy',\n",
    "                       'sentiment':r_squared})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 502,692\n",
      "test: 55,854\n"
     ]
    }
   ],
   "source": [
    "train_pct = 0.90\n",
    "sent = news_data.SENTIMENT.values\n",
    "topic = encoder_topic.transform(news_data.TOPIC.values)\n",
    "\n",
    "X_train, X_test = data[:int(data.shape[0] * train_pct)], \\\n",
    "                  data[-int(data.shape[0] * (1-train_pct)):], \\\n",
    "\n",
    "topic_train, topic_test = topic[:int(data.shape[0] * train_pct)], \\\n",
    "                          topic[-int(data.shape[0] * (1-train_pct)):]\n",
    "\n",
    "sent_train, sent_test = sent[:int(data.shape[0] * train_pct)], \\\n",
    "                        sent[-int(data.shape[0] * (1-train_pct)):]\n",
    "\n",
    "print('train: {:,}\\ntest: {:,}'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 18:09:48.327771 4793939392 deprecation.py:323] From /Users/mkangrga/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 502692 samples, validate on 55854 samples\n",
      "Epoch 1/10\n",
      " 26624/502692 [>.............................] - ETA: 19:21 - loss: 5.4235 - topic_loss: 5.1683 - sentiment_loss: 0.2552 - topic_accuracy: 0.0556 - sentiment_r_squared: -0.0954"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-55edee84dc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           validation_data=(X_test, [topic_test, sent_test]))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, [topic_train, sent_train],\n",
    "          epochs = 10,\n",
    "          batch_size = 1024,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          validation_data=(X_test, [topic_test, sent_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/tf-lstm-model-topic-sent.h5')\n",
    "with open('model/encoder_topic.pkl', 'rb') as f:\n",
    "    encoder_topic = pickle.load(f)\n",
    "    \n",
    "# model.save('model/tf-lstm-model-topic-sent.h5')\n",
    "# with open('model/encoder_topic.pkl', 'wb') as f:\n",
    "#     pickle.dump(encoder_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(txt):\n",
    "    if isinstance(txt, str):\n",
    "        txt = keras.preprocessing.sequence.pad_sequences(\n",
    "            [encode(txt)],\n",
    "            value=tokenizer.vocab[\"<PAD>\"],\n",
    "            padding='post',\n",
    "            maxlen=maxlen)\n",
    "        predictions = model.predict(txt)\n",
    "        topic = encoder_topic.inverse_transform(predictions[0])[0]\n",
    "        sentiment = predictions[1][0][0]\n",
    "    return topic, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('executive-resignation', -0.6195352)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('CEO found guilty of fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('revenue', -0.26360473)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('IBM misses revenue expectations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('analyst-ratings-set', 0.9304361)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''Morgan Stanley analyst Andrew Berens initiates coverage\n",
    "           on Blueprint Medicines (NASDAQ:BPMC) with a Overweight''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('analyst-ratings-change', 0.7553701)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''Senex Energy Upgraded to Outperform from Neutral by Credit Suisse''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('analyst-ratings-change', -0.6919683)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''Qbe Insurance Downgraded to Hold from Buy by Bell Potter''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf-2.0)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "735.972px",
    "left": "899.792px",
    "right": "20px",
    "top": "180px",
    "width": "730.538px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
